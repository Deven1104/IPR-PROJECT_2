# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jBUr84_S4ARlTjBNT2NWocC4DUJ5IyWF
"""

import torch
import torch.nn as nn
from transformers import CLIPModel, CLIPTokenizer
from diffusers import StableDiffusionPipeline
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt

# Check for GPU availability and set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

def calculate_mean_std(image):
    """
    Calculate the mean and standard deviation per channel of an image tensor.

    Args:
        image (torch.Tensor): Image tensor of shape (C, H, W).

    Returns:
        tuple: Mean and standard deviation tensors.
    """
    mean = torch.mean(image, dim=[1, 2])  # Mean across height and width
    std = torch.std(image, dim=[1, 2])    # Std across height and width
    return mean, std

def load_image_as_tensor(image_path):
    """
    Load an image from the specified path, preprocess it, and convert it to a tensor.

    Args:
        image_path (str): Path to the image file.

    Returns:
        torch.Tensor: Preprocessed image tensor with shape (1, C, H, W).
    """
    transform = transforms.Compose([
        transforms.Resize((224, 224)),  # Resize to 224x224 for CLIP
        transforms.ToTensor()
    ])
    image = Image.open(image_path).convert("RGB")
    image = transform(image)

    # Calculate dynamic mean and std for normalization
    mean, std = calculate_mean_std(image)

    # Normalize the image using dynamic mean and std
    normalize = transforms.Normalize(mean.tolist(), std.tolist())
    normalized_image = normalize(image)

    return normalized_image.unsqueeze(0).to(device)  # Add batch dimension and move to device

# Initialize CLIP model and tokenizer
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
text_encoder = clip_model.text_model
image_encoder = clip_model.vision_model
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

# Cell 5: Load Stable Diffusion Model

from diffusers import StableDiffusionPipeline

# Load the Stable Diffusion model from diffusers
diffusion_model = StableDiffusionPipeline.from_pretrained(
    "CompVis/stable-diffusion-v1-4",
    torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32  # Use float16 for faster computation on GPU
).to(device)

# Enable memory-efficient attention if available
if device.type == 'cuda':
    diffusion_model.enable_attention_slicing()  # Reduces memory usage

# Verify the diffusion model is loaded correctly
try:
    test_output = diffusion_model("A test prompt", num_inference_steps=1)
    print("StableDiffusionPipeline loaded successfully.")
except Exception as e:
    print(f"Error loading StableDiffusionPipeline: {e}")

# Cell 6: Define ContextDiffusion Model
class ContextDiffusion(nn.Module):
    def __init__(self, text_encoder, image_encoder, diffusion_model, tokenizer):
        super(ContextDiffusion, self).__init__()
        self.text_encoder = text_encoder.to(device)
        self.image_encoder = image_encoder.to(device)
        self.diffusion_model = diffusion_model  # Already moved to device
        self.tokenizer = tokenizer

        # Add a linear layer to project image embeddings to match text embeddings
        self.image_projection = nn.Linear(768, 512).to(device)

    def forward(self, query_image, context_images, prompt=None):
        # Set encoders to evaluation mode
        self.text_encoder.eval()
        self.image_encoder.eval()
        # Note: Do NOT set diffusion_model to eval() as it causes AttributeError

        with torch.no_grad():
            # Encode the text prompt if available
            if prompt:
                # Tokenize the prompt
                inputs = self.tokenizer(prompt, return_tensors="pt").to(device)
                text_embeddings = self.text_encoder(**inputs).last_hidden_state  # Shape: [1, seq_len, 512]

                # Apply mean pooling to reduce to [1, 512]
                text_embeddings = text_embeddings.mean(dim=1)  # Shape: [1, 512]
                print(f"text_embeddings shape after mean pooling: {text_embeddings.shape}")  # Debug

                # Determine the number of context images
                if isinstance(context_images, list):
                    context_count = len(context_images)
                else:
                    context_count = context_images.size(0)

                # Repeat text_embeddings to match the number of context images
                text_embeddings = text_embeddings.repeat(context_count, 1)  # Shape: [context_count, 512]
                print(f"text_embeddings shape after repeating: {text_embeddings.shape}")  # Debug
            else:
                if isinstance(context_images, list):
                    context_count = len(context_images)
                else:
                    context_count = context_images.size(0)

                text_embeddings = torch.zeros([context_count, 512]).to(device)  # Shape: [context_count, 512]
                print(f"text_embeddings shape (dummy): {text_embeddings.shape}")  # Debug

            # Ensure context_images is a tensor by concatenating if it's a list
            if isinstance(context_images, list):
                context_images = torch.cat(context_images, dim=0).to(device)  # Shape: [context_count, C, H, W]
            else:
                context_images = context_images.to(device)  # Already a tensor
            print(f"context_images shape after concatenation: {context_images.shape}")  # Debug

            # Encode the context images
            context_embeddings = self.image_encoder(pixel_values=context_images).last_hidden_state  # Shape: [context_count, num_patches, 768]
            context_embeddings = context_embeddings.mean(dim=1)  # Shape: [context_count, 768]
            print(f"context_embeddings shape after mean pooling: {context_embeddings.shape}")  # Debug

            # Project image embeddings to match the dimension of text embeddings
            context_embeddings = self.image_projection(context_embeddings)  # Shape: [context_count, 512]
            print(f"context_embeddings shape after projection: {context_embeddings.shape}")  # Debug

            # Combine text and context embeddings
            combined_embeddings = torch.cat([text_embeddings, context_embeddings], dim=1)  # Shape: [context_count, 1024]
            print(f"combined_embeddings shape: {combined_embeddings.shape}")  # Debug

            # Generate an image conditioned on the prompt
            # Note: Currently, the combined_embeddings are not used in the diffusion model.
            # If you intend to use them, further customization is required.
            generated_output = self.diffusion_model(prompt, num_inference_steps=50)

            # Verify that generated_output has the 'images' attribute
            if hasattr(generated_output, 'images') and generated_output.images:
                generated_images = generated_output.images  # List of PIL Images
                print(f"Number of generated images: {len(generated_images)}")  # Debug
                # Return the first image
                return generated_images[0]
            else:
                print("Diffusion model did not return any images.")
                return None

# Instantiate Context Diffusion Model
model = ContextDiffusion(text_encoder, image_encoder, diffusion_model, tokenizer).to(device)

# Cell 8: Load Example Images

# Define paths to your images
query_image_path = "./Images/g.png"
context_image_paths = [
    "./Images/a.png",
   # "./Images/11.png",
    #"./Images/12.png"
]

# Load the query image
query_image = load_image_as_tensor(query_image_path)  # Shape: [1, 3, 224, 224]

# Load the context images
context_images = [load_image_as_tensor(path) for path in context_image_paths]  # List of [1, 3, 224, 224]

# Optionally, verify shapes
print(f"Query image shape: {query_image.shape}")  # Should be [1, 3, 224, 224]
for idx, img in enumerate(context_images):
    print(f"Context image {idx+1} shape: {img.shape}")  # Each should be [1, 3, 224, 224]

# Cell 9: Generate Image with Forward Pass

# Define the text prompt
prompt = "Full moon over our camp, East Greenland"

# Run forward pass to generate the image
try:
    generated_image = model(query_image, context_images, prompt)
    print("Image generation successful.")
    print(f"Type of generated_image: {type(generated_image)}")

    # If it's a PIL Image, print its mode and size
    if isinstance(generated_image, Image.Image):
        print(f"PIL Image mode: {generated_image.mode}")
        print(f"PIL Image size: {generated_image.size}")
    else:
        print("Generated image is not a PIL Image.")
except AttributeError as ae:
    print(f"AttributeError occurred: {ae}")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

# Cell 10: Display the Generated Image

import numpy as np

if generated_image is not None:
    if isinstance(generated_image, Image.Image):
        plt.figure(figsize=(8, 8))
        plt.imshow(generated_image)
        plt.axis('off')
        plt.title("Generated Image")
        plt.show()
    elif isinstance(generated_image, torch.Tensor):
        # Convert Torch Tensor to NumPy Array
        generated_image_np = generated_image.cpu().numpy()
        # If the tensor has a batch dimension, remove it
        if generated_image_np.shape[0] == 1:
            generated_image_np = generated_image_np.squeeze(0)
        # Transpose if necessary (C, H, W) -> (H, W, C)
        if generated_image_np.ndim == 3 and generated_image_np.shape[0] in [1, 3, 4]:
            generated_image_np = np.transpose(generated_image_np, (1, 2, 0))
        plt.figure(figsize=(8, 8))
        plt.imshow(generated_image_np)
        plt.axis('off')
        plt.title("Generated Image")
        plt.show()
    elif isinstance(generated_image, np.ndarray):
        plt.figure(figsize=(8, 8))
        plt.imshow(generated_image)
        plt.axis('off')
        plt.title("Generated Image")
        plt.show()
    else:
        print("Generated image is not in a displayable format.")
else:
    print("No image was generated. Please check the model's forward method for issues.")
